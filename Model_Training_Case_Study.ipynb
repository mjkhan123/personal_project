{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Importing Pandas for Data Operations\n",
    "import pandas as pd\n",
    "import os\n",
    "os.chdir(\"/home/lokeshstat/Downloads\")\n",
    "df=pd.read_csv('data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ptid</th>\n",
       "      <th>Persistency_Flag</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Race</th>\n",
       "      <th>Ethnicity</th>\n",
       "      <th>Region</th>\n",
       "      <th>Age_Bucket</th>\n",
       "      <th>Ntm_Speciality</th>\n",
       "      <th>Ntm_Specialist_Flag</th>\n",
       "      <th>Ntm_Speciality_Bucket</th>\n",
       "      <th>...</th>\n",
       "      <th>Risk_Family_History_Of_Osteoporosis</th>\n",
       "      <th>Risk_Low_Calcium_Intake</th>\n",
       "      <th>Risk_Vitamin_D_Insufficiency</th>\n",
       "      <th>Risk_Poor_Health_Frailty</th>\n",
       "      <th>Risk_Excessive_Thinness</th>\n",
       "      <th>Risk_Hysterectomy_Oophorectomy</th>\n",
       "      <th>Risk_Estrogen_Deficiency</th>\n",
       "      <th>Risk_Immobilization</th>\n",
       "      <th>Risk_Recurring_Falls</th>\n",
       "      <th>Count_Of_Risks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>P1</td>\n",
       "      <td>Persistent</td>\n",
       "      <td>Male</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>Not Hispanic</td>\n",
       "      <td>West</td>\n",
       "      <td>&gt;75</td>\n",
       "      <td>GENERAL PRACTITIONER</td>\n",
       "      <td>Others</td>\n",
       "      <td>OB/GYN/Others/PCP/Unknown</td>\n",
       "      <td>...</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>P2</td>\n",
       "      <td>Non-Persistent</td>\n",
       "      <td>Male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>Not Hispanic</td>\n",
       "      <td>West</td>\n",
       "      <td>55-65</td>\n",
       "      <td>GENERAL PRACTITIONER</td>\n",
       "      <td>Others</td>\n",
       "      <td>OB/GYN/Others/PCP/Unknown</td>\n",
       "      <td>...</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>P3</td>\n",
       "      <td>Non-Persistent</td>\n",
       "      <td>Female</td>\n",
       "      <td>Other/Unknown</td>\n",
       "      <td>Hispanic</td>\n",
       "      <td>Midwest</td>\n",
       "      <td>65-75</td>\n",
       "      <td>GENERAL PRACTITIONER</td>\n",
       "      <td>Others</td>\n",
       "      <td>OB/GYN/Others/PCP/Unknown</td>\n",
       "      <td>...</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>P4</td>\n",
       "      <td>Non-Persistent</td>\n",
       "      <td>Female</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>Not Hispanic</td>\n",
       "      <td>Midwest</td>\n",
       "      <td>&gt;75</td>\n",
       "      <td>GENERAL PRACTITIONER</td>\n",
       "      <td>Others</td>\n",
       "      <td>OB/GYN/Others/PCP/Unknown</td>\n",
       "      <td>...</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>P5</td>\n",
       "      <td>Non-Persistent</td>\n",
       "      <td>Female</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>Not Hispanic</td>\n",
       "      <td>Midwest</td>\n",
       "      <td>&gt;75</td>\n",
       "      <td>GENERAL PRACTITIONER</td>\n",
       "      <td>Others</td>\n",
       "      <td>OB/GYN/Others/PCP/Unknown</td>\n",
       "      <td>...</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 69 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Ptid Persistency_Flag  Gender           Race     Ethnicity   Region  \\\n",
       "0   P1       Persistent    Male      Caucasian  Not Hispanic     West   \n",
       "1   P2   Non-Persistent    Male          Asian  Not Hispanic     West   \n",
       "2   P3   Non-Persistent  Female  Other/Unknown      Hispanic  Midwest   \n",
       "3   P4   Non-Persistent  Female      Caucasian  Not Hispanic  Midwest   \n",
       "4   P5   Non-Persistent  Female      Caucasian  Not Hispanic  Midwest   \n",
       "\n",
       "  Age_Bucket        Ntm_Speciality Ntm_Specialist_Flag  \\\n",
       "0        >75  GENERAL PRACTITIONER              Others   \n",
       "1      55-65  GENERAL PRACTITIONER              Others   \n",
       "2      65-75  GENERAL PRACTITIONER              Others   \n",
       "3        >75  GENERAL PRACTITIONER              Others   \n",
       "4        >75  GENERAL PRACTITIONER              Others   \n",
       "\n",
       "       Ntm_Speciality_Bucket  ... Risk_Family_History_Of_Osteoporosis  \\\n",
       "0  OB/GYN/Others/PCP/Unknown  ...                                   N   \n",
       "1  OB/GYN/Others/PCP/Unknown  ...                                   N   \n",
       "2  OB/GYN/Others/PCP/Unknown  ...                                   N   \n",
       "3  OB/GYN/Others/PCP/Unknown  ...                                   N   \n",
       "4  OB/GYN/Others/PCP/Unknown  ...                                   N   \n",
       "\n",
       "  Risk_Low_Calcium_Intake  Risk_Vitamin_D_Insufficiency  \\\n",
       "0                       N                             N   \n",
       "1                       N                             N   \n",
       "2                       Y                             N   \n",
       "3                       N                             N   \n",
       "4                       N                             N   \n",
       "\n",
       "  Risk_Poor_Health_Frailty Risk_Excessive_Thinness  \\\n",
       "0                        N                       N   \n",
       "1                        N                       N   \n",
       "2                        N                       N   \n",
       "3                        N                       N   \n",
       "4                        N                       N   \n",
       "\n",
       "  Risk_Hysterectomy_Oophorectomy Risk_Estrogen_Deficiency Risk_Immobilization  \\\n",
       "0                              N                        N                   N   \n",
       "1                              N                        N                   N   \n",
       "2                              N                        N                   N   \n",
       "3                              N                        N                   N   \n",
       "4                              N                        N                   N   \n",
       "\n",
       "  Risk_Recurring_Falls Count_Of_Risks  \n",
       "0                    N              0  \n",
       "1                    N              0  \n",
       "2                    N              2  \n",
       "3                    N              1  \n",
       "4                    N              1  \n",
       "\n",
       "[5 rows x 69 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Printing Head of Data Set \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ptid                              0\n",
      "Persistency_Flag                  0\n",
      "Gender                            0\n",
      "Race                              0\n",
      "Ethnicity                         0\n",
      "                                 ..\n",
      "Risk_Hysterectomy_Oophorectomy    0\n",
      "Risk_Estrogen_Deficiency          0\n",
      "Risk_Immobilization               0\n",
      "Risk_Recurring_Falls              0\n",
      "Count_Of_Risks                    0\n",
      "Length: 69, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Calculation of Missing Values\n",
    "df_missing = df.isna()\n",
    "df_missing = df_missing.sum()\n",
    "print(df_missing)\n",
    "df_missing=pd.DataFrame(df_missing)\n",
    "df_missing.to_csv('missing.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     Variable          Category  Frequency  Percentage\n",
      "0                      Gender            Female       3230    0.943341\n",
      "1                      Gender              Male        194    0.056659\n",
      "2                        Race         Caucasian       3148    0.919393\n",
      "3                        Race     Other/Unknown         97    0.028329\n",
      "4                        Race  African American         95    0.027745\n",
      "..                        ...               ...        ...         ...\n",
      "174  Risk_Estrogen_Deficiency                 Y         11    0.003213\n",
      "175       Risk_Immobilization                 N       3410    0.995911\n",
      "176       Risk_Immobilization                 Y         14    0.004089\n",
      "177      Risk_Recurring_Falls                 N       3355    0.979848\n",
      "178      Risk_Recurring_Falls                 Y         69    0.020152\n",
      "\n",
      "[179 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "#Freq Distribution of Cat Variables\n",
    "cat_list=[x for x in df.columns if x not in ['Ptid',\n",
    "'Persistency_Flag',\n",
    "'Count_Of_Risks',\n",
    "'Dexa_Freq_During_Rx',\n",
    "]]\n",
    "\n",
    "freq_list=[]\n",
    "percentage_list=[]\n",
    "var_list=[]\n",
    "header=[]\n",
    "\n",
    "for i in cat_list:\n",
    "    cat_dict=dict(df[i].value_counts())   \n",
    "\n",
    "\n",
    "    for var_cat in cat_dict.keys():\n",
    "        var_list.append(var_cat)\n",
    "\n",
    "    for frequency in cat_dict.values():  \n",
    "        freq_list.append(frequency)  \n",
    "        percentage_list.append(frequency/sum(cat_dict.values()))\n",
    "        header.append(i) \n",
    "        \n",
    "   \n",
    "cat_freq =           {'Variable'  : header,\n",
    "                       'Category'  : var_list,\n",
    "                       'Frequency' : freq_list,\n",
    "                       'Percentage': percentage_list}\n",
    "\n",
    "\n",
    "freq = pd.DataFrame(cat_freq, columns =['Variable','Category','Frequency','Percentage'])\n",
    "\n",
    "\n",
    "freq.to_csv( 'cat_freq.csv',  index = False)        \n",
    "print(freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "drop_list_1=['Ntm_Speciality',\n",
    "'Ntm_Specialist_Flag',\n",
    "'Risk_Osteogenesis_Imperfecta',\n",
    "'Risk_Untreated_Chronic_Hyperthyroidism']\n",
    "\n",
    "df = df.drop(columns = drop_list_1,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Target Mapping\n",
    "mapping_target = {'Persistent':1, 'Non-Persistent':0}\n",
    "df = df.replace({'Persistency_Flag':mapping_target})\n",
    "\n",
    "\n",
    "\n",
    "#Creating intraction Variables\n",
    "df['Gender_Age_Bucket_int']=df['Gender'].astype(str)+\"_\"+df['Age_Bucket'].astype(str)\n",
    "df['Race_Ethnicity_int']=df['Race'].astype(str)+\"_\"+df['Ethnicity'].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#List of Columns where One Hot Encoding is  required\n",
    "ohe_list_1=['Gender',\n",
    "'Age_Bucket',\n",
    "'Race',\n",
    "'Region',\n",
    "'Ethnicity',\n",
    "'Ntm_Speciality_Bucket',\n",
    "'Risk_Segment_Prior_Ntm',\n",
    "'Risk_Segment_During_Rx',\n",
    "'Change_Risk_Segment',\n",
    "'Tscore_Bucket_Prior_Ntm',\n",
    "'Tscore_Bucket_During_Rx',\n",
    "'Change_T_Score',\n",
    "'Gender_Age_Bucket_int',\n",
    "'Race_Ethnicity_int']\n",
    "\n",
    "\n",
    "#one Hot Encoding of Cat columns\n",
    "def one_hot_encoder(df, nan_as_category = False):\n",
    "    original_columns = list(df.columns)\n",
    "    categorical_columns = ohe_list_1\n",
    "    df = pd.get_dummies(df, columns= categorical_columns, dummy_na= nan_as_category)\n",
    "    new_columns = [c for c in df.columns if c not in original_columns]\n",
    "    return df, new_columns\n",
    "\n",
    "df_1, newCols = one_hot_encoder(df, nan_as_category=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Flag Variable creation for dichotomous variables\n",
    "not_in_binary=['Gender',\n",
    "'Age_Bucket',\n",
    "'Race',\n",
    "'Region',\n",
    "'Ethnicity',\n",
    "'Ntm_Speciality_Bucket',\n",
    "'Risk_Segment_Prior_Ntm',\n",
    "'Risk_Segment_During_Rx',\n",
    "'Change_Risk_Segment',\n",
    "'Tscore_Bucket_Prior_Ntm',\n",
    "'Tscore_Bucket_During_Rx',\n",
    "'Change_T_Score',\n",
    "'Ptid',\n",
    "'Persistency_Flag',\n",
    "'Count_Of_Risks',\n",
    "'Dexa_Freq_During_Rx',\n",
    "'Ntm_Speciality',\n",
    "'Ntm_Specialist_Flag',\n",
    "'Risk_Osteogenesis_Imperfecta',\n",
    "'Risk_Untreated_Chronic_Hyperthyroidism',\n",
    "'Gender_Age_Bucket_int',\n",
    "'Race_Ethnicity_int']\n",
    "\n",
    "binary_conversion=[x for x in df.columns if x not in not_in_binary]\n",
    "\n",
    "\n",
    "bin_cnv_y_n= {'Y':1, 'N':0}\n",
    "bin_conv_list=[x for x in binary_conversion if x not in ['Adherent_Flag']]\n",
    "\n",
    "for i in bin_conv_list:\n",
    "    df_1 = df_1.replace({i:bin_cnv_y_n})\n",
    "\n",
    "    \n",
    "bin_cnv_Adherent_Flag= {'Adherent':1, 'Non-Adherent':0}\n",
    "df_1 = df_1.replace({'Adherent_Flag':bin_cnv_Adherent_Flag})    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Removing variables having other/Unkown or Unknown Categoury and variables having more than 99.9% unique Values\n",
    "drop_list_2=['Race_Other/Unknown',\n",
    "'Region_Other/Unknown',\n",
    "'Ethnicity_Unknown',\n",
    "'Risk_Segment_During_Rx_Unknown',\n",
    "'Change_Risk_Segment_Unknown',\n",
    "'Tscore_Bucket_During_Rx_Unknown',\n",
    "'Change_T_Score_Unknown',\n",
    "'Race_Ethnicity_int_African American_Hispanic',\n",
    "'Race_Ethnicity_int_Asian_Hispanic',             \n",
    "'Race_Ethnicity_int_African American_Unknown',\n",
    "'Race_Ethnicity_int_Asian_Unknown',\n",
    "'Race_Ethnicity_int_Caucasian_Unknown',\n",
    "'Race_Ethnicity_int_Other/Unknown_Hispanic',\n",
    "'Race_Ethnicity_int_Other/Unknown_Not Hispanic',\n",
    "'Race_Ethnicity_int_Other/Unknown_Unknown']\n",
    "\n",
    "df_1 = df_1.drop(columns = drop_list_2,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Defining Training columns and target \n",
    "target = 'Persistency_Flag'\n",
    "no_train=['Ptid','Persistency_Flag']\n",
    "trainOn = [x for x in df_1.columns if x not in no_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Removing the column having more than 99.9% unique values\n",
    "toDrop = [x for x in df_1.columns if (max(df_1[x].value_counts()/df_1.shape[0])>0.999)]\n",
    "df_1 = df_1.drop(columns = toDrop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 var    cramer\n",
      "0                             Gluco_Record_Prior_Ntm  0.006392\n",
      "0                             Gluco_Record_During_Rx  0.268500\n",
      "0                                     Dexa_During_Rx  0.621984\n",
      "0                                Frag_Frac_Prior_Ntm  0.000635\n",
      "0                                Frag_Frac_During_Rx  0.134255\n",
      "..                                               ...       ...\n",
      "0                     Gender_Age_Bucket_int_Male_>75  0.000125\n",
      "0   Race_Ethnicity_int_African American_Not Hispanic  0.020907\n",
      "0              Race_Ethnicity_int_Asian_Not Hispanic  0.048084\n",
      "0              Race_Ethnicity_int_Caucasian_Hispanic  0.012146\n",
      "0          Race_Ethnicity_int_Caucasian_Not Hispanic  0.005841\n",
      "\n",
      "[93 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "numerical_list=['Dexa_Freq_During_Rx','Count_Of_Comorbidity','Count_Of_Concomitant_Drug','Count_Of_Risks']\n",
    "import numpy as np \n",
    "import scipy.stats as ss \n",
    "#Defining Cramer's Statistics\n",
    "def cramers_stat(confusion_matrix): \n",
    "    chi2 = ss.chi2_contingency(confusion_matrix)[0]\n",
    "    n = confusion_matrix.sum() \n",
    "    return np.sqrt(chi2 / (n*(min(confusion_matrix.shape)-1))) \n",
    "\n",
    "cramers_list=[x for x in trainOn if x not in numerical_list]\n",
    "\n",
    "#Calculating Cramer's Statistics between Target and Indipendent Variables\n",
    "temp=pd.DataFrame([])\n",
    "for col in cramers_list:\n",
    "    result = cramers_stat(pd.crosstab(df_1[col],df_1[target]))[0:1]\n",
    "    temp=pd.concat([temp,pd.concat([pd.DataFrame([col]), pd.DataFrame([result])],axis=1)],axis=0)\n",
    "\n",
    "temp.columns = ['var', 'cramer']    \n",
    "print(temp)\n",
    "temp.to_csv('Cramer_V_DV_IV.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 var1  \\\n",
      "0               Comorb_Long_Term_Current_Drug_Therapy   \n",
      "0   Comorb_Encounter_For_Screening_For_Malignant_N...   \n",
      "0   Comorb_Encounter_For_Screening_For_Malignant_N...   \n",
      "0                   Comorb_Encounter_For_Immunization   \n",
      "0                   Comorb_Encounter_For_Immunization   \n",
      "..                                                ...   \n",
      "0                              Gluco_Record_During_Rx   \n",
      "0                              Gluco_Record_During_Rx   \n",
      "0                              Gluco_Record_During_Rx   \n",
      "0                              Gluco_Record_During_Rx   \n",
      "0                              Gluco_Record_During_Rx   \n",
      "\n",
      "                                               var2    cramer  \n",
      "0                                    Dexa_During_Rx  0.279843  \n",
      "0                                    Dexa_During_Rx  0.320680  \n",
      "0             Comorb_Long_Term_Current_Drug_Therapy  0.166937  \n",
      "0                                    Dexa_During_Rx  0.258355  \n",
      "0             Comorb_Long_Term_Current_Drug_Therapy  0.231626  \n",
      "..                                              ...       ...  \n",
      "0                                  Concom_Narcotics  0.379316  \n",
      "0                           Concom_Fluoroquinolones  0.282694  \n",
      "0     Comorb_Personal_history_of_malignant_neoplasm  0.098709  \n",
      "0                       Comorb_Vitamin_D_Deficiency  0.094058  \n",
      "0   Ntm_Speciality_Bucket_OB/GYN/Others/PCP/Unknown  0.142095  \n",
      "\n",
      "[276 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "#Selecting Variables where Cramers > 0.2 from the previous step\n",
    "cramers_list_2=['Dexa_During_Rx',\n",
    "'Comorb_Long_Term_Current_Drug_Therapy',\n",
    "'Comorb_Encounter_For_Screening_For_Malignant_Neoplasms',\n",
    "'Comorb_Encounter_For_Immunization',\n",
    "'Comorb_Encntr_For_General_Exam_W_O_Complaint,_Susp_Or_Reprtd_Dx',\n",
    "'Concom_Viral_Vaccines',\n",
    "'Concom_Systemic_Corticosteroids_Plain',\n",
    "'Comorb_Other_Joint_Disorder_Not_Elsewhere_Classified',\n",
    "'Comorb_Gastro_esophageal_reflux_disease',\n",
    "'Comorb_Personal_History_Of_Other_Diseases_And_Conditions',\n",
    "'Comorb_Other_Disorders_Of_Bone_Density_And_Structure',\n",
    "'Concom_Broad_Spectrum_Penicillins',\n",
    "'Concom_Macrolides_And_Similar_Types',\n",
    "'Comorb_Encntr_For_Oth_Sp_Exam_W_O_Complaint_Suspected_Or_Reprtd_Dx',\n",
    "'Concom_Anaesthetics_General',\n",
    "'Concom_Cephalosporins',\n",
    "'Ntm_Speciality_Bucket_Endo/Onc/Uro',\n",
    "'Comorb_Dorsalgia',\n",
    "'Concom_Narcotics',\n",
    "'Concom_Fluoroquinolones',\n",
    "'Comorb_Personal_history_of_malignant_neoplasm',\n",
    "'Comorb_Vitamin_D_Deficiency',\n",
    "'Ntm_Speciality_Bucket_OB/GYN/Others/PCP/Unknown',\n",
    "'Gluco_Record_During_Rx']\n",
    "\n",
    "#Calculating Cramers between indipendent Variables\n",
    "temp=pd.DataFrame([])\n",
    "for col1 in cramers_list_2:\n",
    "    for col2 in cramers_list_2:\n",
    "        if col1==col2:\n",
    "            break\n",
    "        else:\n",
    "            result = cramers_stat(pd.crosstab(df_1[col1],df_1[col2]))[0:1]\n",
    "            temp=pd.concat([temp,pd.concat([pd.DataFrame([col1]),pd.DataFrame([col2]), pd.DataFrame([result])],axis=1)],axis=0)\n",
    "        \n",
    "temp.columns = ['var1','var2', 'cramer'] \n",
    "print(temp)\n",
    "temp.to_csv('Cramer_V_IV_IV.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Dropping Variables having Correlation > =  0.7\n",
    "drop_list=['Ntm_Speciality_Bucket_OB/GYN/Others/PCP/Unknown','Gluco_Record_During_Rx']\n",
    "trainOn=[x for x in cramers_list_2 if x not in drop_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Calculating Fisher Scores for Numerical Variables\n",
    "num_list_fisher=['Dexa_Freq_During_Rx','Count_Of_Risks']\n",
    "temp = pd.DataFrame()\n",
    "for i in num_list_fisher:\n",
    "    mean_nb = df_1.loc[df_1[target] == 0, i].mean()\n",
    "    mean_b = df_1.loc[df_1[target]== 1, i].mean()\n",
    "    var_nb = df_1.loc[df_1[target] == 0, i].var()\n",
    "    var_b = df_1.loc[df_1[target] == 1, i].var()\n",
    "\n",
    "    if (var_nb+var_b==0):\n",
    "        continue\n",
    "    fs = np.abs(mean_nb - mean_b)/np.sqrt(var_nb + var_b)\n",
    "    temp = pd.concat([temp, pd.concat([pd.DataFrame([i]), pd.DataFrame([mean_nb]), pd.DataFrame([mean_b]), pd.DataFrame([var_nb]), pd.DataFrame([var_b]), pd.DataFrame([fs])], axis = 1)])\n",
    "\n",
    "fisher_score = temp\n",
    "fisher_score.columns = ['var', 'mean_nb', 'mean_b', 'var_nb', 'var_b', 'score']\n",
    "fisher_score.to_csv('fisher_score.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             var1                 var2      corr\n",
      "0  Count_Of_Risks  Dexa_Freq_During_Rx  0.013964\n"
     ]
    }
   ],
   "source": [
    "#Caculating Correlation Between Numerical Variables\n",
    "temp=pd.DataFrame([])\n",
    "for col1 in num_list_fisher:\n",
    "    for col2 in num_list_fisher:\n",
    "        if col1==col2:\n",
    "            break\n",
    "        else:\n",
    "            result = np.corrcoef(df_1[col1],df_1[col2]).min()\n",
    "            temp=pd.concat([temp,pd.concat([pd.DataFrame([col1]),pd.DataFrame([col2]), pd.DataFrame([result])],axis=1)],axis=0)\n",
    "\n",
    "temp.columns = ['var1','var2', 'corr']\n",
    "print(temp)\n",
    "temp.to_csv('corr.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Selecting final Variables\n",
    "final_model_var=['Concom_Broad_Spectrum_Penicillins',\n",
    "'Comorb_Other_Joint_Disorder_Not_Elsewhere_Classified',\n",
    "'Comorb_Encounter_For_Immunization',\n",
    "'Comorb_Gastro_esophageal_reflux_disease',\n",
    "'Comorb_Encntr_For_General_Exam_W_O_Complaint,_Susp_Or_Reprtd_Dx',\n",
    "'Concom_Viral_Vaccines',\n",
    "'Comorb_Vitamin_D_Deficiency',\n",
    "'Comorb_Encounter_For_Screening_For_Malignant_Neoplasms',\n",
    "'Ntm_Speciality_Bucket_Endo/Onc/Uro',\n",
    "'Comorb_Long_Term_Current_Drug_Therapy',\n",
    "'Dexa_During_Rx']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bagging_fraction', 'colsample_bytree', 'feature_fraction', 'learning_rate', 'max_depth', 'min_child_samples', 'min_child_weight', 'min_data_in_leaf', 'num_leaves', 'reg_alpha', 'reg_lambda', 'scale_pos_weight', 'subsample']\n",
      "----------------------------------------------------------------------------------------------------------------------------------\n",
      "|   iter    |  target   | baggin... | colsam... | featur... | learni... | max_depth | min_ch... | min_ch... | min_da... | num_le... | reg_alpha | reg_la... | scale_... | subsample |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.8513  \u001b[0m | \u001b[0m 0.3996  \u001b[0m | \u001b[0m 0.7803  \u001b[0m | \u001b[0m 0.6856  \u001b[0m | \u001b[0m 0.04592 \u001b[0m | \u001b[0m 1.496   \u001b[0m | \u001b[0m 86.44   \u001b[0m | \u001b[0m 0.000590\u001b[0m | \u001b[0m 130.6   \u001b[0m | \u001b[0m 30.85   \u001b[0m | \u001b[0m 7.373   \u001b[0m | \u001b[0m 1.185   \u001b[0m | \u001b[0m 97.02   \u001b[0m | \u001b[0m 0.6995  \u001b[0m |\n",
      "| \u001b[95m 2       \u001b[0m | \u001b[95m 0.8525  \u001b[0m | \u001b[95m 0.2699  \u001b[0m | \u001b[95m 0.4727  \u001b[0m | \u001b[95m 0.2467  \u001b[0m | \u001b[95m 0.02825 \u001b[0m | \u001b[95m 7.396   \u001b[0m | \u001b[95m 221.7   \u001b[0m | \u001b[95m 0.002919\u001b[0m | \u001b[95m 93.72   \u001b[0m | \u001b[95m 8.696   \u001b[0m | \u001b[95m 3.629   \u001b[0m | \u001b[95m 4.297   \u001b[0m | \u001b[95m 46.15   \u001b[0m | \u001b[95m 0.6711  \u001b[0m |\n",
      "| \u001b[0m 3       \u001b[0m | \u001b[0m 0.8476  \u001b[0m | \u001b[0m 0.2597  \u001b[0m | \u001b[0m 0.6057  \u001b[0m | \u001b[0m 0.5739  \u001b[0m | \u001b[0m 0.01279 \u001b[0m | \u001b[0m 8.721   \u001b[0m | \u001b[0m 93.56   \u001b[0m | \u001b[0m 0.000659\u001b[0m | \u001b[0m 142.6   \u001b[0m | \u001b[0m 48.35   \u001b[0m | \u001b[0m 8.276   \u001b[0m | \u001b[0m 3.742   \u001b[0m | \u001b[0m 10.67   \u001b[0m | \u001b[0m 0.6105  \u001b[0m |\n",
      "| \u001b[0m 4       \u001b[0m | \u001b[0m 0.8498  \u001b[0m | \u001b[0m 0.4521  \u001b[0m | \u001b[0m 0.4488  \u001b[0m | \u001b[0m 0.4961  \u001b[0m | \u001b[0m 0.01206 \u001b[0m | \u001b[0m 13.55   \u001b[0m | \u001b[0m 136.8   \u001b[0m | \u001b[0m 0.006629\u001b[0m | \u001b[0m 50.2    \u001b[0m | \u001b[0m 26.96   \u001b[0m | \u001b[0m 5.92    \u001b[0m | \u001b[0m 2.664   \u001b[0m | \u001b[0m 96.99   \u001b[0m | \u001b[0m 0.6651  \u001b[0m |\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m 0.8496  \u001b[0m | \u001b[0m 0.8516  \u001b[0m | \u001b[0m 0.7579  \u001b[0m | \u001b[0m 0.5783  \u001b[0m | \u001b[0m 0.06531 \u001b[0m | \u001b[0m 0.4159  \u001b[0m | \u001b[0m 106.0   \u001b[0m | \u001b[0m 0.000461\u001b[0m | \u001b[0m 52.17   \u001b[0m | \u001b[0m 20.66   \u001b[0m | \u001b[0m 3.442   \u001b[0m | \u001b[0m 8.459   \u001b[0m | \u001b[0m 36.32   \u001b[0m | \u001b[0m 0.3686  \u001b[0m |\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 0.5342  \u001b[0m | \u001b[0m 0.4564  \u001b[0m | \u001b[0m 0.7418  \u001b[0m | \u001b[0m 0.01447 \u001b[0m | \u001b[0m 14.79   \u001b[0m | \u001b[0m 388.4   \u001b[0m | \u001b[0m 0.001995\u001b[0m | \u001b[0m 5.801   \u001b[0m | \u001b[0m 41.14   \u001b[0m | \u001b[0m 7.362   \u001b[0m | \u001b[0m 7.561   \u001b[0m | \u001b[0m 77.36   \u001b[0m | \u001b[0m 0.2444  \u001b[0m |\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m 0.8471  \u001b[0m | \u001b[0m 0.3868  \u001b[0m | \u001b[0m 0.4463  \u001b[0m | \u001b[0m 0.7905  \u001b[0m | \u001b[0m 0.0474  \u001b[0m | \u001b[0m 4.294   \u001b[0m | \u001b[0m 41.14   \u001b[0m | \u001b[0m 0.003117\u001b[0m | \u001b[0m 52.15   \u001b[0m | \u001b[0m 37.02   \u001b[0m | \u001b[0m 6.738   \u001b[0m | \u001b[0m 8.985   \u001b[0m | \u001b[0m 47.75   \u001b[0m | \u001b[0m 0.2718  \u001b[0m |\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m 0.8487  \u001b[0m | \u001b[0m 0.6706  \u001b[0m | \u001b[0m 0.7043  \u001b[0m | \u001b[0m 0.549   \u001b[0m | \u001b[0m 0.05626 \u001b[0m | \u001b[0m 6.901   \u001b[0m | \u001b[0m 266.1   \u001b[0m | \u001b[0m 0.004281\u001b[0m | \u001b[0m 8.686   \u001b[0m | \u001b[0m 7.179   \u001b[0m | \u001b[0m 1.283   \u001b[0m | \u001b[0m 6.728   \u001b[0m | \u001b[0m 32.12   \u001b[0m | \u001b[0m 0.5051  \u001b[0m |\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 0.8261  \u001b[0m | \u001b[0m 0.4997  \u001b[0m | \u001b[0m 0.4283  \u001b[0m | \u001b[0m 0.05533 \u001b[0m | \u001b[0m 2.661   \u001b[0m | \u001b[0m 47.72   \u001b[0m | \u001b[0m 0.002905\u001b[0m | \u001b[0m 28.38   \u001b[0m | \u001b[0m 46.63   \u001b[0m | \u001b[0m 8.273   \u001b[0m | \u001b[0m 6.701   \u001b[0m | \u001b[0m 87.27   \u001b[0m | \u001b[0m 0.6822  \u001b[0m |\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m 0.85    \u001b[0m | \u001b[0m 0.2493  \u001b[0m | \u001b[0m 0.757   \u001b[0m | \u001b[0m 0.5315  \u001b[0m | \u001b[0m 0.05845 \u001b[0m | \u001b[0m 13.34   \u001b[0m | \u001b[0m 165.8   \u001b[0m | \u001b[0m 0.001109\u001b[0m | \u001b[0m 38.05   \u001b[0m | \u001b[0m 22.5    \u001b[0m | \u001b[0m 8.362   \u001b[0m | \u001b[0m 8.747   \u001b[0m | \u001b[0m 1.688   \u001b[0m | \u001b[0m 0.5064  \u001b[0m |\n",
      "| \u001b[95m 11      \u001b[0m | \u001b[95m 0.8527  \u001b[0m | \u001b[95m 0.4339  \u001b[0m | \u001b[95m 0.4888  \u001b[0m | \u001b[95m 0.1959  \u001b[0m | \u001b[95m 0.03026 \u001b[0m | \u001b[95m 14.09   \u001b[0m | \u001b[95m 168.4   \u001b[0m | \u001b[95m 0.005193\u001b[0m | \u001b[95m 106.9   \u001b[0m | \u001b[95m 19.45   \u001b[0m | \u001b[95m 9.746   \u001b[0m | \u001b[95m 9.662   \u001b[0m | \u001b[95m 25.93   \u001b[0m | \u001b[95m 0.4983  \u001b[0m |\n",
      "| \u001b[0m 12      \u001b[0m | \u001b[0m 0.8464  \u001b[0m | \u001b[0m 0.3407  \u001b[0m | \u001b[0m 0.5139  \u001b[0m | \u001b[0m 0.1295  \u001b[0m | \u001b[0m 0.04657 \u001b[0m | \u001b[0m 7.043   \u001b[0m | \u001b[0m 35.22   \u001b[0m | \u001b[0m 0.002794\u001b[0m | \u001b[0m 136.7   \u001b[0m | \u001b[0m 13.5    \u001b[0m | \u001b[0m 2.304   \u001b[0m | \u001b[0m 5.405   \u001b[0m | \u001b[0m 98.58   \u001b[0m | \u001b[0m 0.3452  \u001b[0m |\n",
      "| \u001b[0m 13      \u001b[0m | \u001b[0m 0.8523  \u001b[0m | \u001b[0m 0.6377  \u001b[0m | \u001b[0m 0.7046  \u001b[0m | \u001b[0m 0.2901  \u001b[0m | \u001b[0m 0.05369 \u001b[0m | \u001b[0m 4.885   \u001b[0m | \u001b[0m 319.8   \u001b[0m | \u001b[0m 0.006339\u001b[0m | \u001b[0m 82.69   \u001b[0m | \u001b[0m 6.334   \u001b[0m | \u001b[0m 8.518   \u001b[0m | \u001b[0m 3.887   \u001b[0m | \u001b[0m 19.47   \u001b[0m | \u001b[0m 0.2245  \u001b[0m |\n",
      "| \u001b[95m 14      \u001b[0m | \u001b[95m 0.8531  \u001b[0m | \u001b[95m 0.5727  \u001b[0m | \u001b[95m 0.671   \u001b[0m | \u001b[95m 0.1133  \u001b[0m | \u001b[95m 0.04073 \u001b[0m | \u001b[95m 2.624   \u001b[0m | \u001b[95m 326.1   \u001b[0m | \u001b[95m 0.001752\u001b[0m | \u001b[95m 105.2   \u001b[0m | \u001b[95m 20.56   \u001b[0m | \u001b[95m 9.431   \u001b[0m | \u001b[95m 2.238   \u001b[0m | \u001b[95m 34.77   \u001b[0m | \u001b[95m 0.2681  \u001b[0m |\n",
      "| \u001b[0m 15      \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 0.8398  \u001b[0m | \u001b[0m 0.7509  \u001b[0m | \u001b[0m 0.3064  \u001b[0m | \u001b[0m 0.0496  \u001b[0m | \u001b[0m 12.08   \u001b[0m | \u001b[0m 282.0   \u001b[0m | \u001b[0m 0.005301\u001b[0m | \u001b[0m 40.07   \u001b[0m | \u001b[0m 6.469   \u001b[0m | \u001b[0m 9.075   \u001b[0m | \u001b[0m 9.104   \u001b[0m | \u001b[0m 63.68   \u001b[0m | \u001b[0m 0.4034  \u001b[0m |\n",
      "| \u001b[0m 16      \u001b[0m | \u001b[0m 0.8516  \u001b[0m | \u001b[0m 0.1     \u001b[0m | \u001b[0m 0.4     \u001b[0m | \u001b[0m 0.9     \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m-1.0     \u001b[0m | \u001b[0m 500.0   \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 150.0   \u001b[0m | \u001b[0m 50.0    \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 0.8     \u001b[0m |\n",
      "| \u001b[0m 17      \u001b[0m | \u001b[0m 0.8519  \u001b[0m | \u001b[0m 0.1     \u001b[0m | \u001b[0m 0.8     \u001b[0m | \u001b[0m 0.1     \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m-1.0     \u001b[0m | \u001b[0m 500.0   \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 0.8     \u001b[0m |\n",
      "| \u001b[0m 18      \u001b[0m | \u001b[0m 0.8425  \u001b[0m | \u001b[0m 0.1     \u001b[0m | \u001b[0m 0.4     \u001b[0m | \u001b[0m 0.1     \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 15.0    \u001b[0m | \u001b[0m 500.0   \u001b[0m | \u001b[0m 1e-05   \u001b[0m | \u001b[0m 150.0   \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 100.0   \u001b[0m | \u001b[0m 0.2     \u001b[0m |\n",
      "| \u001b[0m 19      \u001b[0m | \u001b[0m 0.8485  \u001b[0m | \u001b[0m 0.5696  \u001b[0m | \u001b[0m 0.5629  \u001b[0m | \u001b[0m 0.6542  \u001b[0m | \u001b[0m 0.01971 \u001b[0m | \u001b[0m 8.31    \u001b[0m | \u001b[0m 324.4   \u001b[0m | \u001b[0m 0.009991\u001b[0m | \u001b[0m 98.7    \u001b[0m | \u001b[0m 17.29   \u001b[0m | \u001b[0m 9.306   \u001b[0m | \u001b[0m 4.317   \u001b[0m | \u001b[0m 19.19   \u001b[0m | \u001b[0m 0.2088  \u001b[0m |\n",
      "| \u001b[0m 20      \u001b[0m | \u001b[0m 0.8519  \u001b[0m | \u001b[0m 0.1     \u001b[0m | \u001b[0m 0.4     \u001b[0m | \u001b[0m 0.9     \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 15.0    \u001b[0m | \u001b[0m 10.0    \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 0.8     \u001b[0m |\n",
      "| \u001b[0m 21      \u001b[0m | \u001b[0m 0.8519  \u001b[0m | \u001b[0m 0.1     \u001b[0m | \u001b[0m 0.4     \u001b[0m | \u001b[0m 0.9     \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 15.0    \u001b[0m | \u001b[0m 10.0    \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 150.0   \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 10.0    \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 0.2     \u001b[0m |\n",
      "| \u001b[0m 22      \u001b[0m | \u001b[0m 0.8495  \u001b[0m | \u001b[0m 0.213   \u001b[0m | \u001b[0m 0.649   \u001b[0m | \u001b[0m 0.3434  \u001b[0m | \u001b[0m 0.01929 \u001b[0m | \u001b[0m 6.323   \u001b[0m | \u001b[0m 218.5   \u001b[0m | \u001b[0m 0.007397\u001b[0m | \u001b[0m 89.63   \u001b[0m | \u001b[0m 5.605   \u001b[0m | \u001b[0m 1.756   \u001b[0m | \u001b[0m 2.694   \u001b[0m | \u001b[0m 47.04   \u001b[0m | \u001b[0m 0.2798  \u001b[0m |\n",
      "| \u001b[0m 23      \u001b[0m | \u001b[0m 0.8502  \u001b[0m | \u001b[0m 0.2827  \u001b[0m | \u001b[0m 0.7736  \u001b[0m | \u001b[0m 0.8372  \u001b[0m | \u001b[0m 0.05351 \u001b[0m | \u001b[0m 8.753   \u001b[0m | \u001b[0m 168.2   \u001b[0m | \u001b[0m 0.001874\u001b[0m | \u001b[0m 39.84   \u001b[0m | \u001b[0m 19.69   \u001b[0m | \u001b[0m 6.462   \u001b[0m | \u001b[0m 4.578   \u001b[0m | \u001b[0m 1.717   \u001b[0m | \u001b[0m 0.2869  \u001b[0m |\n",
      "| \u001b[95m 24      \u001b[0m | \u001b[95m 0.8541  \u001b[0m | \u001b[95m 0.1     \u001b[0m | \u001b[95m 0.8     \u001b[0m | \u001b[95m 0.1     \u001b[0m | \u001b[95m 0.07    \u001b[0m | \u001b[95m-1.0     \u001b[0m | \u001b[95m 325.0   \u001b[0m | \u001b[95m 1e-05   \u001b[0m | \u001b[95m 5.0     \u001b[0m | \u001b[95m 50.0    \u001b[0m | \u001b[95m 1.0     \u001b[0m | \u001b[95m 1.0     \u001b[0m | \u001b[95m 1.0     \u001b[0m | \u001b[95m 0.8     \u001b[0m |\n",
      "| \u001b[0m 25      \u001b[0m | \u001b[0m 0.8523  \u001b[0m | \u001b[0m 0.1     \u001b[0m | \u001b[0m 0.4     \u001b[0m | \u001b[0m 0.1     \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m-1.0     \u001b[0m | \u001b[0m 281.2   \u001b[0m | \u001b[0m 1e-05   \u001b[0m | \u001b[0m 150.0   \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 10.0    \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 0.3563  \u001b[0m |\n",
      "=====================================================================================================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'bagging_fraction': 0.1,\n",
       " 'colsample_bytree': 0.7999999998367054,\n",
       " 'feature_fraction': 0.1,\n",
       " 'learning_rate': 0.06999999986765802,\n",
       " 'max_depth': -1.0,\n",
       " 'min_child_samples': 325.01757892868,\n",
       " 'min_child_weight': 1e-05,\n",
       " 'min_data_in_leaf': 5.0,\n",
       " 'num_leaves': 50.0,\n",
       " 'reg_alpha': 1.0,\n",
       " 'reg_lambda': 1.0,\n",
       " 'scale_pos_weight': 1.0,\n",
       " 'subsample': 0.7999999995425457}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split, StratifiedKFold,KFold\n",
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn.metrics import precision_score, recall_score, confusion_matrix, accuracy_score, roc_auc_score, f1_score, roc_curve, auc\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "import lightgbm as lgb\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import itertools\n",
    "from scipy import interp\n",
    "\n",
    "\n",
    "train_df=df_1\n",
    "features=final_model_var\n",
    "target = 'Persistency_Flag'\n",
    "\n",
    "#cut tr and val\n",
    "bayesian_tr_idx, bayesian_val_idx = train_test_split(train_df[features], test_size = 0.2, random_state = 42, stratify = train_df[target])\n",
    "bayesian_tr_idx = bayesian_tr_idx.index\n",
    "bayesian_val_idx = bayesian_val_idx.index\n",
    "\n",
    "\n",
    "\n",
    "#black box LGBM \n",
    "def LGB_bayesian(\n",
    "    learning_rate,\n",
    "    num_leaves, \n",
    "    bagging_fraction,\n",
    "    feature_fraction,\n",
    "    min_child_samples, \n",
    "    min_child_weight,\n",
    "    subsample, \n",
    "    min_data_in_leaf,\n",
    "    max_depth,\n",
    "    colsample_bytree,\n",
    "    reg_alpha,\n",
    "    reg_lambda,\n",
    "    scale_pos_weight\n",
    "     ):\n",
    "    \n",
    "    # LightGBM expects next three parameters need to be integer. \n",
    "    num_leaves = int(num_leaves)\n",
    "    min_data_in_leaf = int(min_data_in_leaf)\n",
    "    max_depth = int(max_depth)\n",
    "\n",
    "    assert type(num_leaves) == int\n",
    "    assert type(min_data_in_leaf) == int\n",
    "    assert type(max_depth) == int\n",
    "    \n",
    "\n",
    "    param = {\n",
    "              'num_leaves': num_leaves, \n",
    "              'min_child_samples': min_child_samples, \n",
    "              'min_data_in_leaf': min_data_in_leaf,\n",
    "              'min_child_weight': min_child_weight,\n",
    "              'bagging_fraction' : bagging_fraction,\n",
    "              'feature_fraction' : feature_fraction,\n",
    "              'subsample': subsample, \n",
    "              'max_depth': max_depth,\n",
    "              'colsample_bytree': colsample_bytree,\n",
    "              'reg_alpha': reg_alpha,\n",
    "              'reg_lambda': reg_lambda,\n",
    "              'scale_pos_weight':scale_pos_weight,\n",
    "              'objective': 'binary',\n",
    "              'save_binary': True,\n",
    "              'seed': 1337,\n",
    "              'feature_fraction_seed': 1337,\n",
    "              'bagging_seed': 1337,\n",
    "              'drop_seed': 1337,\n",
    "              'data_random_seed': 1337,\n",
    "              'boosting_type': 'gbdt',\n",
    "              'verbose': 1,\n",
    "#              'is_unbalance': False,\n",
    "              'boost_from_average': True,\n",
    "              'metric':'auc'}    \n",
    "    \n",
    "    oof = np.zeros(len(train_df))\n",
    "    categorical = []\n",
    "\n",
    "    trn_data= lgb.Dataset(train_df.iloc[bayesian_tr_idx][features].values, label=train_df.iloc[bayesian_tr_idx][target].values)\n",
    "    val_data= lgb.Dataset(train_df.iloc[bayesian_val_idx][features].values, label=train_df.iloc[bayesian_val_idx][target].values)\n",
    "\n",
    "    clf = lgb.train(param, trn_data,  num_boost_round=100000, valid_sets = [trn_data, val_data], verbose_eval=0, early_stopping_rounds = 50)\n",
    "    \n",
    "    oof[bayesian_val_idx]  = clf.predict(train_df.iloc[bayesian_val_idx][features].values, num_iteration=clf.best_iteration)  \n",
    "    \n",
    "    score = roc_auc_score(train_df.iloc[bayesian_val_idx][target].values, oof[bayesian_val_idx])\n",
    "\n",
    "    return score\n",
    "\n",
    "\n",
    "# Bounded region of parameter space\n",
    "bounds_LGB = {\n",
    "    'num_leaves': (2, 50), \n",
    "    'min_data_in_leaf': (5, 150),\n",
    "    'bagging_fraction' : (0.1,0.9),\n",
    "    'feature_fraction' : (0.1,0.9),\n",
    "    'learning_rate': (0.01, 0.07),\n",
    "    'min_child_weight': (0.00001, 0.01),   \n",
    "    'min_child_samples':(10, 500), \n",
    "    'subsample': (0.2, 0.8),\n",
    "    'colsample_bytree': (0.4, 0.8), \n",
    "    'reg_alpha': (1, 10), \n",
    "    'reg_lambda': (1, 10),\n",
    "    'max_depth':(-1,15),\n",
    "    'scale_pos_weight':(1,100)\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "LGB_BO = BayesianOptimization(LGB_bayesian, bounds_LGB, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "print(LGB_BO.space.keys)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "init_points = 15\n",
    "n_iter = 10\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('-' * 130)\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings('ignore')\n",
    "    LGB_BO.maximize(init_points=init_points, n_iter=n_iter, acq='ucb', xi=0.0, alpha=1e-6)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "LGB_BO.max['target']\n",
    "LGB_BO.max['params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 400 rounds\n",
      "[100]\ttraining's auc: 0.867908\ttraining's binary_logloss: 0.438945\tvalid_1's auc: 0.844273\tvalid_1's binary_logloss: 0.458667\n",
      "[200]\ttraining's auc: 0.869678\ttraining's binary_logloss: 0.430331\tvalid_1's auc: 0.84612\tvalid_1's binary_logloss: 0.453662\n",
      "[300]\ttraining's auc: 0.870218\ttraining's binary_logloss: 0.428837\tvalid_1's auc: 0.846555\tvalid_1's binary_logloss: 0.452963\n",
      "[400]\ttraining's auc: 0.870612\ttraining's binary_logloss: 0.428309\tvalid_1's auc: 0.846591\tvalid_1's binary_logloss: 0.452483\n",
      "Early stopping, best iteration is:\n",
      "[21]\ttraining's auc: 0.86191\ttraining's binary_logloss: 0.526094\tvalid_1's auc: 0.848403\tvalid_1's binary_logloss: 0.531382\n"
     ]
    }
   ],
   "source": [
    "X = df_1[final_model_var]\n",
    "y = df_1[target]\n",
    "\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "train_x, valid_x, train_y, valid_y = train_test_split(X[final_model_var], y, test_size=0.1, shuffle=True, stratify=y, random_state=9001)\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "lgbc=LGBMClassifier(n_estimators=10000,\n",
    "                    bagging_fraction= 0.1,\n",
    "                     colsample_bytree= 0.7999999998367054,\n",
    "                     feature_fraction= 0.1,\n",
    "                     learning_rate= 0.06999999986765802,\n",
    "                     max_depth= -1,\n",
    "                     min_child_samples= 325.01757892868,\n",
    "                     min_child_weight= 1e-05,\n",
    "                     min_data_in_leaf= 5,\n",
    "                     num_leaves=50,\n",
    "                     reg_alpha= 1.0,\n",
    "                     reg_lambda= 1.0,\n",
    "                     scale_pos_weight= 1,\n",
    "                     subsample= 0.7999999995425457)\n",
    "\n",
    "lgbc.fit(train_x, train_y, eval_set=[(train_x, train_y), \n",
    "                                     (valid_x, valid_y)],eval_metric= 'auc', verbose= 100, \n",
    "                                         early_stopping_rounds= 400,\n",
    "                                         )\n",
    "lgb_importance_df = pd.DataFrame()\n",
    "lgb_importance_df[\"feature\"] = final_model_var\n",
    "lgb_importance_df[\"importance\"] = lgbc.feature_importances_*(100)\n",
    "\n",
    "lgbc.best_iteration_\n",
    "\n",
    "lgb_importance_df.to_excel('fi_lgb.xlsx',index=False)\n",
    "\n",
    "\n",
    "sub_preds = np.zeros(df.shape[0])\n",
    "sub_preds = lgbc.predict_proba(df_1[final_model_var], num_iteration=lgbc.best_iteration_)[:, 1] \n",
    "df['score'] = sub_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-auc:0.824742\tvalidation_1-auc:0.792201\n",
      "Multiple eval metrics have been passed: 'validation_1-auc' will be used for early stopping.\n",
      "\n",
      "Will train until validation_1-auc hasn't improved in 200 rounds.\n",
      "[100]\tvalidation_0-auc:0.851241\tvalidation_1-auc:0.809697\n",
      "[200]\tvalidation_0-auc:0.859552\tvalidation_1-auc:0.822285\n",
      "[300]\tvalidation_0-auc:0.865205\tvalidation_1-auc:0.824241\n",
      "[400]\tvalidation_0-auc:0.868373\tvalidation_1-auc:0.828153\n",
      "[500]\tvalidation_0-auc:0.869817\tvalidation_1-auc:0.832899\n",
      "[600]\tvalidation_0-auc:0.871005\tvalidation_1-auc:0.835289\n",
      "[700]\tvalidation_0-auc:0.871616\tvalidation_1-auc:0.837101\n",
      "[800]\tvalidation_0-auc:0.872063\tvalidation_1-auc:0.838187\n",
      "[900]\tvalidation_0-auc:0.872633\tvalidation_1-auc:0.839165\n",
      "[1000]\tvalidation_0-auc:0.872906\tvalidation_1-auc:0.840107\n",
      "[1100]\tvalidation_0-auc:0.873033\tvalidation_1-auc:0.840252\n",
      "[1200]\tvalidation_0-auc:0.873087\tvalidation_1-auc:0.841411\n",
      "[1300]\tvalidation_0-auc:0.873353\tvalidation_1-auc:0.842027\n",
      "[1400]\tvalidation_0-auc:0.873388\tvalidation_1-auc:0.842172\n",
      "[1500]\tvalidation_0-auc:0.87355\tvalidation_1-auc:0.842389\n",
      "[1600]\tvalidation_0-auc:0.873622\tvalidation_1-auc:0.842281\n",
      "[1700]\tvalidation_0-auc:0.873719\tvalidation_1-auc:0.842317\n",
      "[1800]\tvalidation_0-auc:0.873752\tvalidation_1-auc:0.842281\n",
      "Stopping. Best iteration:\n",
      "[1605]\tvalidation_0-auc:0.873659\tvalidation_1-auc:0.842462\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = df_1[final_model_var]\n",
    "y = df_1[target]\n",
    "\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "train_x, valid_x, train_y, valid_y = train_test_split(X[final_model_var], y, test_size=0.1, shuffle=True, stratify=y, random_state=9001)\n",
    "\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "xgbc=XGBClassifier(n_estimators=10000, \n",
    "                   learning_rate=0.01, \n",
    "                   num_leaves=50, \n",
    "                   colsample_bytree=1,\n",
    "                   reg_alpha=0.2, \n",
    "                   reg_lambda=50, \n",
    "                   min_split_gain=0.5, \n",
    "                   min_child_weight=30)\n",
    "\n",
    "xgbc.fit(train_x, train_y, eval_set=[(train_x, train_y), (valid_x, valid_y)],eval_metric= 'auc', verbose= 100, early_stopping_rounds= 200)\n",
    "xgb_importance_df = pd.DataFrame()\n",
    "xgb_importance_df[\"feature\"] = final_model_var\n",
    "xgb_importance_df[\"importance\"] = xgbc.feature_importances_*(100)\n",
    "\n",
    "xgbc.best_iteration\n",
    "\n",
    "xgb_importance_df.to_excel('fi_xgb_oot.xlsx',index=False)\n",
    "\n",
    "\n",
    "sub_preds = np.zeros(df.shape[0])\n",
    "sub_preds = xgbc.predict_proba(df_1[final_model_var], xgbc.best_iteration)[:, 1] \n",
    "df['score'] = sub_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion_matrix:  Predicted     0    1\n",
      "Actual              \n",
      "0          1575  346\n",
      "1           259  901\n",
      "roc_auc_score:  0.798304807122727\n",
      "Precision:  0.7225340817963112\n",
      "Recall:  0.7767241379310345\n",
      "F1 score:  0.7486497714997923\n"
     ]
    }
   ],
   "source": [
    "#Model Results For Training Data\n",
    "train_pred = xgbc.predict_proba(train_x[final_model_var], xgbc.best_iteration)[:, 1] \n",
    "train_pred = np.where(train_pred < 0.38, 0, 1)\n",
    "confusion_matrix = pd.crosstab(train_y, train_pred, rownames=['Actual'], colnames=['Predicted'])\n",
    "print('confusion_matrix: ',confusion_matrix)\n",
    "from sklearn.metrics import roc_auc_score,precision_recall_fscore_support\n",
    "print('roc_auc_score: ',roc_auc_score(train_y, train_pred))\n",
    "precision, recall, f1_score, _ = precision_recall_fscore_support(train_y, train_pred, average='binary')\n",
    "print(\"Precision: \", precision)\n",
    "print(\"Recall: \", recall)\n",
    "print(\"F1 score: \", f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion_matrix:  Predicted    0   1\n",
      "Actual            \n",
      "0          170  44\n",
      "1           36  93\n",
      "roc_auc_score:  0.7576613779613127\n",
      "Precision:  0.6788321167883211\n",
      "Recall:  0.7209302325581395\n",
      "F1 score:  0.6992481203007519\n"
     ]
    }
   ],
   "source": [
    "#Model Results For Test Data\n",
    "test_pred =  lgbc.predict_proba(valid_x[final_model_var],num_iteration=lgbc.best_iteration_)[:, 1] \n",
    "test_pred = np.where(test_pred < 0.38, 0, 1)\n",
    "confusion_matrix = pd.crosstab(valid_y, test_pred, rownames=['Actual'], colnames=['Predicted'])\n",
    "\n",
    "print('confusion_matrix: ',confusion_matrix)\n",
    "from sklearn.metrics import roc_auc_score,precision_recall_fscore_support\n",
    "print('roc_auc_score: ',roc_auc_score(valid_y, test_pred))\n",
    "precision, recall, f1_score, _ = precision_recall_fscore_support(valid_y, test_pred, average='binary')\n",
    "print(\"Precision: \", precision)\n",
    "print(\"Recall: \", recall)\n",
    "print(\"F1 score: \", f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion_matrix:  Predicted     0    1\n",
      "Actual              \n",
      "0          1745  390\n",
      "1           296  993\n",
      "roc_auc_score:  0.7938474172560833\n",
      "Precision:  0.7180043383947939\n",
      "Recall:  0.7703646237393328\n",
      "F1 score:  0.7432634730538923\n"
     ]
    }
   ],
   "source": [
    "#Model Results For Overall Data\n",
    "oa_pred =  xgbc.predict_proba(df_1[final_model_var], xgbc.best_iteration)[:, 1] \n",
    "oa_pred = np.where(oa_pred < 0.38, 0, 1)\n",
    "confusion_matrix = pd.crosstab(df_1[target], oa_pred, rownames=['Actual'], colnames=['Predicted'])\n",
    "\n",
    "print('confusion_matrix: ',confusion_matrix)\n",
    "from sklearn.metrics import roc_auc_score,precision_recall_fscore_support\n",
    "print('roc_auc_score: ',roc_auc_score(df_1[target], oa_pred))\n",
    "precision, recall, f1_score, _ = precision_recall_fscore_support(df_1[target], oa_pred, average='binary')\n",
    "print(\"Precision: \", precision)\n",
    "print(\"Recall: \", recall)\n",
    "print(\"F1 score: \", f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1745  390]\n",
      " [ 296  993]]\n",
      "[2135 1289] [2041 1383] 3424\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, brier_score_loss, accuracy_score\n",
    "\n",
    "def show_all(df, nrow=None, ncol=None):\n",
    "    with pd.option_context('display.max_rows', nrow, 'display.max_columns', ncol):\n",
    "        display(df)\n",
    "        \n",
    "def plot_roc(actual, predicted, fig_name='roc_curve'):\n",
    "\tfpr, tpr, threshold = metrics.roc_curve(actual, predicted)\n",
    "\troc_auc = metrics.auc(fpr, tpr)\n",
    "\tplt.clf()\n",
    "\tplt.plot(fpr,tpr,color='red',label='AUC=%0.2f'%roc_auc)\n",
    "\tplt.plot([0,1],[0,1],'k')\n",
    "\tplt.xlim([0.0,1.0])\n",
    "\tplt.ylim([0.0,1.0])\n",
    "\tplt.grid()\n",
    "\tplt.xlabel('False Positive Rate')\n",
    "\tplt.ylabel('True Positive Rate')\n",
    "\ttitleStr='ROC Curve'\n",
    "\tplt.title(titleStr)\n",
    "\tplt.legend(loc=\"lower right\")\n",
    "\tplt.savefig(fig_name)\n",
    "    \n",
    "def get_decile_table(df, dv='dv', score='score'):\n",
    "\tdf = df[[dv, score]].copy()\n",
    "\tdf.sort_values(score, ascending=False, inplace = True)\n",
    "\tdf['Decile'] = pd.qcut(np.arange(1, df.shape[0]+1, 1), 10, labels=False)\n",
    "\tdf['Decile'] = df['Decile'] + 1\n",
    "\tbrier_losses = df.groupby(['Decile']).apply(lambda x: brier_score_loss(x[dv], x[score]))\n",
    "\t\n",
    "\tdecl_tbl = pd.crosstab(df['Decile'], df[dv])\n",
    "\t\n",
    "\tif 0 not in decl_tbl.columns.tolist():\n",
    "\t\tdecl_tbl[0] = 0\n",
    "\t\tdecl_tbl = decl_tbl[[0, 1]]\n",
    "\telif 1 not in decl_tbl.columns.tolist():\n",
    "\t\tdecl_tbl[1] = 0\n",
    "\n",
    "\tdecl_tbl.columns = ['Non_Event', 'Event']\n",
    "\tdecl_tbl.reset_index(inplace=True)\n",
    "\tdecl_tbl = decl_tbl[['Decile', 'Event', 'Non_Event']]\n",
    "\tdecl_tbl['Total'] = decl_tbl['Event'] + decl_tbl['Non_Event']\n",
    "\tdecl_tbl['Cum_Event'] = np.cumsum(decl_tbl['Event'])\n",
    "\tdecl_tbl['Cum_Non_Event'] = np.cumsum(decl_tbl['Non_Event'])\n",
    "\tdecl_tbl['Cum_Total'] = np.cumsum(decl_tbl['Total'])\n",
    "\tdecl_tbl['Cum_Event_Prop'] = decl_tbl['Cum_Event']/decl_tbl['Cum_Event'].max()\n",
    "\tdecl_tbl['Cum_Non_Event_Prop'] = decl_tbl['Cum_Non_Event']/decl_tbl['Cum_Non_Event'].max()\n",
    "\tdecl_tbl['Cum_Lift_Event'] = decl_tbl['Cum_Event_Prop']/(decl_tbl['Decile']/10)\n",
    "\tdecl_tbl['KS'] = decl_tbl['Cum_Event_Prop'] - decl_tbl['Cum_Non_Event_Prop']\n",
    "\tdecl_tbl['Brier_Score_Loss'] = brier_losses\n",
    "\treturn decl_tbl\n",
    "\n",
    "\n",
    "def get_decile_lift_curve(decl_tbl=None, df=None, dv='dv', score='score'):\n",
    "\tif decl_tbl is None:\n",
    "\t\tdecl_tbl = get_decile_table(df, dv, score)\n",
    "\t\t\n",
    "\treturn decl_tbl['Decile'].values, decl_tbl['Cum_Lift_Event'].values    \n",
    "\n",
    "\n",
    "\n",
    "def plot_decile_lift_curve(decl_tbl=None, df=None, dv='dv', score='score', fig_name=\"decile_lift\"):\n",
    "\tdeciles, lifts = get_decile_lift_curve(decl_tbl=decl_tbl, df=df, dv=dv, score=score)\n",
    "\tplt.clf()\n",
    "\tplt.plot(deciles, lifts)\n",
    "\tplt.xlabel(\"Deciles\")\n",
    "\tplt.ylabel(\"Cum Lift of an Event\")\n",
    "\tplt.title(\"Decile Lift\")\n",
    "\tplt.savefig(fig_name)\n",
    "    \n",
    "def get_confusion_matrix(df, prob_thresh = 0.5, labels=[0, 1], dv='dv', score='score'):\n",
    "\tcon_mat = confusion_matrix(df[dv], df[score] > prob_thresh, labels=labels)\n",
    "\treturn con_mat\n",
    "\n",
    "\n",
    "plot_roc(df_1[target], oa_pred, 'roc')\n",
    "\n",
    "\n",
    "oa_pred_1=xgbc.predict_proba(df_1[final_model_var], xgbc.best_iteration)[:, 1]\n",
    "df_1['score']=oa_pred_1\n",
    "x = get_confusion_matrix(df_1, prob_thresh =0.38, labels=[0, 1],dv=target, score='score')\n",
    "print(x)\n",
    "print(x.sum(axis=1), x.sum(axis=0), x.sum())\n",
    "\n",
    "\n",
    "decl_table=get_decile_table(df_1, dv=target, score='score')\n",
    "\n",
    "decl_table.to_csv('decl_table.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Decile</th>\n",
       "      <th>Event</th>\n",
       "      <th>Non_Event</th>\n",
       "      <th>Total</th>\n",
       "      <th>Cum_Event</th>\n",
       "      <th>Cum_Non_Event</th>\n",
       "      <th>Cum_Total</th>\n",
       "      <th>Cum_Event_Prop</th>\n",
       "      <th>Cum_Non_Event_Prop</th>\n",
       "      <th>Cum_Lift_Event</th>\n",
       "      <th>KS</th>\n",
       "      <th>Brier_Score_Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>329</td>\n",
       "      <td>14</td>\n",
       "      <td>343</td>\n",
       "      <td>329</td>\n",
       "      <td>14</td>\n",
       "      <td>343</td>\n",
       "      <td>0.255237</td>\n",
       "      <td>0.006557</td>\n",
       "      <td>2.552366</td>\n",
       "      <td>0.248679</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>275</td>\n",
       "      <td>67</td>\n",
       "      <td>342</td>\n",
       "      <td>604</td>\n",
       "      <td>81</td>\n",
       "      <td>685</td>\n",
       "      <td>0.468580</td>\n",
       "      <td>0.037939</td>\n",
       "      <td>2.342901</td>\n",
       "      <td>0.430641</td>\n",
       "      <td>0.040240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>230</td>\n",
       "      <td>112</td>\n",
       "      <td>342</td>\n",
       "      <td>834</td>\n",
       "      <td>193</td>\n",
       "      <td>1027</td>\n",
       "      <td>0.647013</td>\n",
       "      <td>0.090398</td>\n",
       "      <td>2.156711</td>\n",
       "      <td>0.556615</td>\n",
       "      <td>0.156417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>156</td>\n",
       "      <td>187</td>\n",
       "      <td>343</td>\n",
       "      <td>990</td>\n",
       "      <td>380</td>\n",
       "      <td>1370</td>\n",
       "      <td>0.768037</td>\n",
       "      <td>0.177986</td>\n",
       "      <td>1.920093</td>\n",
       "      <td>0.590051</td>\n",
       "      <td>0.215652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>108</td>\n",
       "      <td>234</td>\n",
       "      <td>342</td>\n",
       "      <td>1098</td>\n",
       "      <td>614</td>\n",
       "      <td>1712</td>\n",
       "      <td>0.851823</td>\n",
       "      <td>0.287588</td>\n",
       "      <td>1.703646</td>\n",
       "      <td>0.564235</td>\n",
       "      <td>0.247822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>68</td>\n",
       "      <td>274</td>\n",
       "      <td>342</td>\n",
       "      <td>1166</td>\n",
       "      <td>888</td>\n",
       "      <td>2054</td>\n",
       "      <td>0.904577</td>\n",
       "      <td>0.415925</td>\n",
       "      <td>1.507629</td>\n",
       "      <td>0.488652</td>\n",
       "      <td>0.216077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>48</td>\n",
       "      <td>295</td>\n",
       "      <td>343</td>\n",
       "      <td>1214</td>\n",
       "      <td>1183</td>\n",
       "      <td>2397</td>\n",
       "      <td>0.941815</td>\n",
       "      <td>0.554098</td>\n",
       "      <td>1.345451</td>\n",
       "      <td>0.387717</td>\n",
       "      <td>0.159644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>35</td>\n",
       "      <td>307</td>\n",
       "      <td>342</td>\n",
       "      <td>1249</td>\n",
       "      <td>1490</td>\n",
       "      <td>2739</td>\n",
       "      <td>0.968968</td>\n",
       "      <td>0.697892</td>\n",
       "      <td>1.211210</td>\n",
       "      <td>0.271076</td>\n",
       "      <td>0.120629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>23</td>\n",
       "      <td>319</td>\n",
       "      <td>342</td>\n",
       "      <td>1272</td>\n",
       "      <td>1809</td>\n",
       "      <td>3081</td>\n",
       "      <td>0.986811</td>\n",
       "      <td>0.847307</td>\n",
       "      <td>1.096457</td>\n",
       "      <td>0.139505</td>\n",
       "      <td>0.091606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>17</td>\n",
       "      <td>326</td>\n",
       "      <td>343</td>\n",
       "      <td>1289</td>\n",
       "      <td>2135</td>\n",
       "      <td>3424</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.062102</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Decile  Event  Non_Event  Total  Cum_Event  Cum_Non_Event  Cum_Total  \\\n",
       "0       1    329         14    343        329             14        343   \n",
       "1       2    275         67    342        604             81        685   \n",
       "2       3    230        112    342        834            193       1027   \n",
       "3       4    156        187    343        990            380       1370   \n",
       "4       5    108        234    342       1098            614       1712   \n",
       "5       6     68        274    342       1166            888       2054   \n",
       "6       7     48        295    343       1214           1183       2397   \n",
       "7       8     35        307    342       1249           1490       2739   \n",
       "8       9     23        319    342       1272           1809       3081   \n",
       "9      10     17        326    343       1289           2135       3424   \n",
       "\n",
       "   Cum_Event_Prop  Cum_Non_Event_Prop  Cum_Lift_Event        KS  \\\n",
       "0        0.255237            0.006557        2.552366  0.248679   \n",
       "1        0.468580            0.037939        2.342901  0.430641   \n",
       "2        0.647013            0.090398        2.156711  0.556615   \n",
       "3        0.768037            0.177986        1.920093  0.590051   \n",
       "4        0.851823            0.287588        1.703646  0.564235   \n",
       "5        0.904577            0.415925        1.507629  0.488652   \n",
       "6        0.941815            0.554098        1.345451  0.387717   \n",
       "7        0.968968            0.697892        1.211210  0.271076   \n",
       "8        0.986811            0.847307        1.096457  0.139505   \n",
       "9        1.000000            1.000000        1.000000  0.000000   \n",
       "\n",
       "   Brier_Score_Loss  \n",
       "0               NaN  \n",
       "1          0.040240  \n",
       "2          0.156417  \n",
       "3          0.215652  \n",
       "4          0.247822  \n",
       "5          0.216077  \n",
       "6          0.159644  \n",
       "7          0.120629  \n",
       "8          0.091606  \n",
       "9          0.062102  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decl_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_decile_lift_curve(df=df_1, dv=target, score='score', fig_name=\"decile_lift\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_df['pre1'] = [1 if (value >= 0.375)  else 0 for value in all_df['prediction']]\n",
    "# x=all_df.groupby([ 'cat'])[[dv, 'score_0_1']].agg({dv: ['count', 'mean'], 'score_0_1': ['mean']})\n",
    "# x.to_csv('cat.csv')\n",
    "\n",
    "\n",
    "# # num\n",
    "\n",
    "# #age\n",
    "# all_df['age'].fillna(0,inplace=True)\n",
    "# t = pd.qcut(all_df['age'], q=10, labels=None, retbins=True, duplicates= 'drop')\n",
    "# all_df['age'] = t[0]\n",
    "# x = all_df.groupby(['age'])[[dv, 'score_0_1']].mean().reset_index()\n",
    "# x.to_csv('age.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
