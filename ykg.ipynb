{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Transformation\n",
    "for column in numerical_list:\n",
    "    df[column +'_sqrt']=np.sqrt(1 + df[column].values).astype(float)\n",
    "    oot[column +'_sqrt']=np.sqrt(1 + oot[column].values).astype(float)\n",
    "    \n",
    "for column in numerical_list:\n",
    "    df[column +'_log']=np.log2(1 + df[column].values).astype(float)\n",
    "    oot[column +'_log']=np.log2(1 + oot[column].values).astype(float) \n",
    "    \n",
    "for column in numerical_list:\n",
    "    df[column +'_inv']=(1 /(1+df[column].values)).astype(float)\n",
    "    oot[column +'_inv']=(1 /(1+oot[column].values)).astype(float)\n",
    "\n",
    "  \n",
    "oot=oot.replace([np.inf, -np.inf], np.nan)\n",
    "df=df.replace([np.inf, -np.inf], np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train Test Split and outlier Treatment\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_x, valid_x, train_y, valid_y = train_test_split(X, y, test_size=0.15, shuffle=True, stratify=y, random_state=10001)\n",
    "\n",
    "for col in numerical_list:\n",
    "    percentiles = train_x[col].quantile([0.01,0.99]).values\n",
    "    train_x[col][train_x[col] <= percentiles[0]] = percentiles[0]\n",
    "    train_x[col][train_x[col] >= percentiles[1]] = percentiles[1]\n",
    "\n",
    "    valid_x[col][valid_x[col] <= percentiles[0]] = percentiles[0]\n",
    "    valid_x[col][valid_x[col] >= percentiles[1]] = percentiles[1]    \n",
    "    \n",
    "    oot1[col][oot1[col] <= percentiles[0]] = percentiles[0]\n",
    "    oot1[col][oot1[col] >= percentiles[1]] = percentiles[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Missing Value Treatment\n",
    "mtreat=[]\n",
    "for column in mtreat:\n",
    "    train_x[column]=train_x[column].fillna(train_x[column].mean())  \n",
    "    valid_x[column]=valid_x[column].fillna(train_x[column].mean()) \n",
    "    oot1[column]=oot1[column].fillna(train_x[column].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Date Part Variables\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "\n",
    "\n",
    "date_part_data = df\n",
    "\n",
    "#input Data Frame\n",
    "#Date Variable\n",
    "#Unique ID\n",
    "#time = True/False(Date Variable has time stamp or not)\n",
    "#start_end= True/False(if True it will add month start, Month End, year_start, year_end fratures)\n",
    "\n",
    "def DatePart(df, DATE,UID,time=False,start_end=True):\n",
    "    df_output=df[[DATE,UID]]\n",
    "    if isinstance(DATE,str): \n",
    "        DATE = [DATE]\n",
    "    for date_part_name in DATE:\n",
    "        fld = df_output[date_part_name]\n",
    "        date_part_dtype = fld.dtype\n",
    "        if isinstance(date_part_dtype, pd.core.dtypes.dtypes.DatetimeTZDtype):\n",
    "            date_part_dtype = np.datetime64\n",
    "            \n",
    "        if not np.issubdtype(date_part_dtype, np.datetime64):\n",
    "                df_output[date_part_name] = fld = pd.to_datetime(fld, infer_datetime_format=True)\n",
    "        DateColName =date_part_name\n",
    "        levl = ['Quarter','Year', 'Month', 'Week', 'Day', 'Dayofweek', 'Dayofyear','Weekday']\n",
    "        start_end=['Is_month_end', 'Is_month_start', 'Is_quarter_end','Is_quarter_start', 'Is_year_end', 'Is_year_start','Is_Leap_Year']\n",
    "        if time: levl = levl + ['Hour', 'Minute', 'Second']\n",
    "        if start_end: levl = levl + start_end\n",
    "        for n in levl: df_output[DateColName +\"_\"+ n] = getattr(fld.dt, n.lower()).astype(int)\n",
    "        df_output.drop(date_part_name, axis=1, inplace=True)\n",
    "        df_output[DateColName +\"_\"+'Is_Weekend'] = [1 if value > 4  else 0 for value in df_output[DateColName +\"_\"+'Day']]\n",
    "        df_output[DateColName +\"_\"+'Is_WeekStart'] = [1 if value < 2  else 0 for value in df_output[DateColName +\"_\"+'Day']]\n",
    "        df_output=pd.merge(df,df_output,on=UID,how='inner')\n",
    "    return df_output\n",
    "        \n",
    "Date_Variable_Data=DatePart(date_part_data,'date','uid',time=True,start_end=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Aggregation Variables\n",
    "\n",
    "cat=['gender','network',]\n",
    "num=['age','income']\n",
    "\n",
    "\n",
    "uid='recid'\n",
    "\n",
    "\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "def agg(num_col, num_rows = None, nan_as_category = True):\n",
    "    agg_data = df1\n",
    "    aggregations = {\n",
    "                num_col:['mean','std','max','min','skew','median']                        \n",
    "                }\n",
    "    agg_data_agg = agg_data.groupby(column).agg(aggregations)\n",
    "    agg_data_agg.columns = ([column +\"_\"+ e[0] + \"_\" + e[1] for e in agg_data_agg.columns.tolist()])\n",
    "    gc.collect()\n",
    "    return agg_data_agg\n",
    "\n",
    "\n",
    "\n",
    "for column in cat:\n",
    "    for column1 in num:\n",
    "        agg_data =agg(column1)\n",
    "#        print(agg_data.columns)\n",
    "\n",
    "        df1 = pd.merge(agg_data, df1, on = [column], how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, brier_score_loss, accuracy_score\n",
    "\n",
    "def show_all(df, nrow=None, ncol=None):\n",
    "    with pd.option_context('display.max_rows', nrow, 'display.max_columns', ncol):\n",
    "        display(df)\n",
    "        \n",
    "def plot_roc(actual, predicted, fig_name='roc_curve'):\n",
    "\tfpr, tpr, threshold = metrics.roc_curve(actual, predicted)\n",
    "\troc_auc = metrics.auc(fpr, tpr)\n",
    "\tplt.clf()\n",
    "\tplt.plot(fpr,tpr,color='red',label='AUC=%0.2f'%roc_auc)\n",
    "\tplt.plot([0,1],[0,1],'k')\n",
    "\tplt.xlim([0.0,1.0])\n",
    "\tplt.ylim([0.0,1.0])\n",
    "\tplt.grid()\n",
    "\tplt.xlabel('False Positive Rate')\n",
    "\tplt.ylabel('True Positive Rate')\n",
    "\ttitleStr='ROC Curve'\n",
    "\tplt.title(titleStr)\n",
    "\tplt.legend(loc=\"lower right\")\n",
    "\tplt.savefig(fig_name)\n",
    "    \n",
    "def get_decile_table(df, dv='dv', score='score'):\n",
    "\tdf = df[[dv, score]].copy()\n",
    "\tdf.sort_values(score, ascending=False, inplace = True)\n",
    "\tdf['Decile'] = pd.qcut(np.arange(1, df.shape[0]+1, 1), 10, labels=False)\n",
    "\tdf['Decile'] = df['Decile'] + 1\n",
    "\tbrier_losses = df.groupby(['Decile']).apply(lambda x: brier_score_loss(x[dv], x[score]))\n",
    "\t\n",
    "\tdecl_tbl = pd.crosstab(df['Decile'], df[dv])\n",
    "\t\n",
    "\tif 0 not in decl_tbl.columns.tolist():\n",
    "\t\tdecl_tbl[0] = 0\n",
    "\t\tdecl_tbl = decl_tbl[[0, 1]]\n",
    "\telif 1 not in decl_tbl.columns.tolist():\n",
    "\t\tdecl_tbl[1] = 0\n",
    "\n",
    "\tdecl_tbl.columns = ['Non_Event', 'Event']\n",
    "\tdecl_tbl.reset_index(inplace=True)\n",
    "\tdecl_tbl = decl_tbl[['Decile', 'Event', 'Non_Event']]\n",
    "\tdecl_tbl['Total'] = decl_tbl['Event'] + decl_tbl['Non_Event']\n",
    "\tdecl_tbl['Cum_Event'] = np.cumsum(decl_tbl['Event'])\n",
    "\tdecl_tbl['Cum_Non_Event'] = np.cumsum(decl_tbl['Non_Event'])\n",
    "\tdecl_tbl['Cum_Total'] = np.cumsum(decl_tbl['Total'])\n",
    "\tdecl_tbl['Cum_Event_Prop'] = decl_tbl['Cum_Event']/decl_tbl['Cum_Event'].max()\n",
    "\tdecl_tbl['Cum_Non_Event_Prop'] = decl_tbl['Cum_Non_Event']/decl_tbl['Cum_Non_Event'].max()\n",
    "\tdecl_tbl['Cum_Lift_Event'] = decl_tbl['Cum_Event_Prop']/(decl_tbl['Decile']/10)\n",
    "\tdecl_tbl['KS'] = decl_tbl['Cum_Event_Prop'] - decl_tbl['Cum_Non_Event_Prop']\n",
    "\tdecl_tbl['Brier_Score_Loss'] = brier_losses\n",
    "\treturn decl_tbl\n",
    "\n",
    "\n",
    "def get_decile_lift_curve(decl_tbl=None, df=None, dv='dv', score='score'):\n",
    "\tif decl_tbl is None:\n",
    "\t\tdecl_tbl = get_decile_table(df, dv, score)\n",
    "\t\t\n",
    "\treturn decl_tbl['Decile'].values, decl_tbl['Cum_Lift_Event'].values    \n",
    "\n",
    "\n",
    "\n",
    "def plot_decile_lift_curve(decl_tbl=None, df=None, dv='dv', score='score', fig_name=\"decile_lift\"):\n",
    "\tdeciles, lifts = get_decile_lift_curve(decl_tbl=decl_tbl, df=df, dv=dv, score=score)\n",
    "\tplt.clf()\n",
    "\tplt.plot(deciles, lifts)\n",
    "\tplt.xlabel(\"Deciles\")\n",
    "\tplt.ylabel(\"Cum Lift of an Event\")\n",
    "\tplt.title(\"Decile Lift\")\n",
    "\tplt.savefig(fig_name)\n",
    "    \n",
    "def get_confusion_matrix(df, prob_thresh = 0.5, labels=[0, 1], dv='dv', score='score'):\n",
    "\tcon_mat = confusion_matrix(df[dv], df[score] > prob_thresh, labels=labels)\n",
    "\treturn con_mat\n",
    "\n",
    "\n",
    "plot_roc(jul_19['life_lead'], jul_19['prediction'], 'jul_19')\n",
    "\n",
    "\n",
    "\n",
    "x = get_confusion_matrix(jul_19, prob_thresh =0.33, labels=[0, 1], dv='life_lead', score='prediction')\n",
    "print(x)\n",
    "print(x.sum(axis=1), x.sum(axis=0), x.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#groupby\n",
    "temp = df1.groupby(by = 'homeno').agg({'life_sms' : 'sum'})\n",
    "temp2 = pd.DataFrame()\n",
    "temp2['homeno'] = temp.index\n",
    "temp2['sms_cnt_workno'] = temp.life_sms.values\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "act_17=alls[alls['DOL'] >= '2018-01-01'] ;gc.collect()\n",
    "act_17_1=act_17[act_17['DOJ'] < '2018-01-01'];gc.collect()\n",
    "act_17_1['Process_Type']=act_17_1['Process_Type'].str.upper()\n",
    "act_17_1['Pay_Code']=act_17_1['Pay_Code'].str.upper()\n",
    "act_17_2=act_17_1[act_17_1['Pay_Code'] != 'GRATUITY'] ;gc.collect()\n",
    "act_17_3=act_17_2[act_17_2['Process_Type'] != 'FANDF'] ;gc.collect()\n",
    "july_1 = pd.to_datetime('2018-07-01', format = '%Y-%m-%d')\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pandasql import *\n",
    "pysqldf = lambda r: sqldf(r, globals())\n",
    "r  =\"\"\" select \"\"\"\n",
    "act_17_uni = pysqldf(r)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bayesian-optimization\n",
      "  Downloading https://files.pythonhosted.org/packages/bb/7a/fd8059a3881d3ab37ac8f72f56b73937a14e8bb14a9733e68cc8b17dbe3c/bayesian-optimization-1.2.0.tar.gz\n",
      "Requirement already satisfied: numpy>=1.9.0 in /home/lokeshstat/anaconda3/lib/python3.6/site-packages (from bayesian-optimization) (1.17.3)\n",
      "Requirement already satisfied: scipy>=0.14.0 in /home/lokeshstat/anaconda3/lib/python3.6/site-packages (from bayesian-optimization) (0.19.1)\n",
      "Requirement already satisfied: scikit-learn>=0.18.0 in /home/lokeshstat/anaconda3/lib/python3.6/site-packages (from bayesian-optimization) (0.19.1)\n",
      "Building wheels for collected packages: bayesian-optimization\n",
      "  Building wheel for bayesian-optimization (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for bayesian-optimization: filename=bayesian_optimization-1.2.0-cp36-none-any.whl size=12607 sha256=1727ad956a7737da314d6b1f15b1eb5fcab6488e960263df922d918503ca64c8\n",
      "  Stored in directory: /home/lokeshstat/.cache/pip/wheels/5a/56/ae/e0e3c1fc1954dc3ec712e2df547235ed072b448094d8f94aec\n",
      "Successfully built bayesian-optimization\n",
      "Installing collected packages: bayesian-optimization\n",
      "Successfully installed bayesian-optimization-1.2.0\n",
      "\u001b[33mWARNING: You are using pip version 19.3.1; however, version 20.1.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install bayesian-optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-ec5bd3eb0c4e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mtrain_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "#  Libraries\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "# Data processing, metrics and modeling\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold,KFold\n",
    "from bayes_opt import BayesianOptimization\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import precision_score, recall_score, confusion_matrix, accuracy_score, roc_auc_score, f1_score, roc_curve, auc\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "# Lgbm\n",
    "import lightgbm as lgb\n",
    "# Suppr warning\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import itertools\n",
    "from scipy import interp\n",
    "\n",
    "# Plots\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "\n",
    "\n",
    "train_df=X_train\n",
    "test_df=X_test\n",
    "\n",
    "features=trainOn\n",
    "\n",
    "target = target\n",
    "\n",
    "\n",
    "\n",
    "#cut tr and val\n",
    "bayesian_tr_idx, bayesian_val_idx = train_test_split(train_df[features], test_size = 0.2, random_state = 42, stratify = train_df[target])\n",
    "bayesian_tr_idx = bayesian_tr_idx.index\n",
    "bayesian_val_idx = bayesian_val_idx.index\n",
    "\n",
    "\n",
    "#categorical_list = []\n",
    "#numerical_list = []\n",
    "#for i in train_df.columns.tolist():\n",
    "#    if train_df[i].dtype=='object':\n",
    "#        categorical_list.append(i)\n",
    "#   \n",
    "#print('Number of categorical features:', str(len(categorical_list)))\n",
    "#print('Number of numerical features:', str(len(numerical_list)))\n",
    "\n",
    "\n",
    "\n",
    "#black box LGBM \n",
    "def LGB_bayesian(\n",
    "    learning_rate,\n",
    "    num_leaves, \n",
    "    bagging_fraction,\n",
    "    feature_fraction,\n",
    "    min_child_samples, \n",
    "    min_child_weight,\n",
    "    subsample, \n",
    "    min_data_in_leaf,\n",
    "    max_depth,\n",
    "    colsample_bytree,\n",
    "    reg_alpha,\n",
    "    reg_lambda,\n",
    "    scale_pos_weight\n",
    "     ):\n",
    "    \n",
    "    # LightGBM expects next three parameters need to be integer. \n",
    "    num_leaves = int(num_leaves)\n",
    "    min_data_in_leaf = int(min_data_in_leaf)\n",
    "    max_depth = int(max_depth)\n",
    "\n",
    "    assert type(num_leaves) == int\n",
    "    assert type(min_data_in_leaf) == int\n",
    "    assert type(max_depth) == int\n",
    "    \n",
    "\n",
    "    param = {\n",
    "              'num_leaves': num_leaves, \n",
    "              'min_child_samples': min_child_samples, \n",
    "              'min_data_in_leaf': min_data_in_leaf,\n",
    "              'min_child_weight': min_child_weight,\n",
    "              'bagging_fraction' : bagging_fraction,\n",
    "              'feature_fraction' : feature_fraction,\n",
    "              'subsample': subsample, \n",
    "              'max_depth': max_depth,\n",
    "              'colsample_bytree': colsample_bytree,\n",
    "              'reg_alpha': reg_alpha,\n",
    "              'reg_lambda': reg_lambda,\n",
    "              'scale_pos_weight':scale_pos_weight,\n",
    "              'objective': 'binary',\n",
    "              'save_binary': True,\n",
    "              'seed': 1337,\n",
    "              'feature_fraction_seed': 1337,\n",
    "              'bagging_seed': 1337,\n",
    "              'drop_seed': 1337,\n",
    "              'data_random_seed': 1337,\n",
    "              'boosting_type': 'gbdt',\n",
    "              'verbose': 1,\n",
    "#              'is_unbalance': False,\n",
    "              'boost_from_average': True,\n",
    "              'metric':'auc'}    \n",
    "    \n",
    "    oof = np.zeros(len(train_df))\n",
    "    categorical = ['app_code','hour', 'day']\n",
    "\n",
    "    trn_data= lgb.Dataset(train_df.iloc[bayesian_tr_idx][features].values, label=train_df.iloc[bayesian_tr_idx][target].values)\n",
    "    val_data= lgb.Dataset(train_df.iloc[bayesian_val_idx][features].values, label=train_df.iloc[bayesian_val_idx][target].values)\n",
    "\n",
    "    clf = lgb.train(param, trn_data,  num_boost_round=100000, valid_sets = [trn_data, val_data], verbose_eval=0, early_stopping_rounds = 50)\n",
    "    \n",
    "    oof[bayesian_val_idx]  = clf.predict(train_df.iloc[bayesian_val_idx][features].values, num_iteration=clf.best_iteration)  \n",
    "    \n",
    "    score = roc_auc_score(train_df.iloc[bayesian_val_idx][target].values, oof[bayesian_val_idx])\n",
    "\n",
    "    return score\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Bounded region of parameter space\n",
    "bounds_LGB = {\n",
    "    'num_leaves': (2, 50), \n",
    "    'min_data_in_leaf': (5, 150),\n",
    "    'bagging_fraction' : (0.1,0.9),\n",
    "    'feature_fraction' : (0.1,0.9),\n",
    "    'learning_rate': (0.01, 0.07),\n",
    "    'min_child_weight': (0.00001, 0.01),   \n",
    "    'min_child_samples':(10, 500), \n",
    "    'subsample': (0.2, 0.8),\n",
    "    'colsample_bytree': (0.4, 0.8), \n",
    "    'reg_alpha': (1, 10), \n",
    "    'reg_lambda': (1, 10),\n",
    "    'max_depth':(-1,15),\n",
    "    'scale_pos_weight':(1,100)\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "LGB_BO = BayesianOptimization(LGB_bayesian, bounds_LGB, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "print(LGB_BO.space.keys)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "init_points = 15\n",
    "n_iter = 10\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('-' * 130)\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings('ignore')\n",
    "    LGB_BO.maximize(init_points=init_points, n_iter=n_iter, acq='ucb', xi=0.0, alpha=1e-6)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "LGB_BO.max['target']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "LGB_BO.max['params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = oot[numerical_list]\n",
    "y = oot[dv]\n",
    "\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "train_x, valid_x, train_y, valid_y = train_test_split(X[numerical_list], y, test_size=0.1, shuffle=True, stratify=y, random_state=9001)\n",
    "from xgboost import XGBClassifier\n",
    "xgbc=XGBClassifier(n_estimators=10000, \n",
    "                   learning_rate=0.01, \n",
    "                   num_leaves=2, \n",
    "                   colsample_bytree=1,\n",
    "                   reg_alpha=0, \n",
    "                   reg_lambda=50, \n",
    "                   min_split_gain=0.5, \n",
    "                   min_child_weight=30)\n",
    "\n",
    "xgbc.fit(train_x, train_y, eval_set=[(train_x, train_y), (valid_x, valid_y)],eval_metric= 'auc', verbose= 100, early_stopping_rounds= 200)\n",
    "xgb_importance_df = pd.DataFrame()\n",
    "xgb_importance_df[\"feature\"] = numerical_list\n",
    "xgb_importance_df[\"importance\"] = xgbc.feature_importances_*(100)\n",
    "\n",
    "xgbc.best_iteration\n",
    "\n",
    "xgb_importance_df.to_excel('fi_xgb_oot.xlsx',index=False)\n",
    "\n",
    "\n",
    "sub_preds = np.zeros(df.shape[0])\n",
    "sub_preds = xgbc.predict_proba(df[numerical_list], xgbc.best_iteration)[:, 1] \n",
    "df['score'] = sub_preds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
